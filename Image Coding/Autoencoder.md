# Autoencoder

## ğŸ§  What is an Autoencoder?
- An Autoencoder is a type of **neural network** used to **compress and reconstruct data** â€” usually **unsupervised (no labels required)**.

![Autoencoder](https://i-blog.csdnimg.cn/blog_migrate/708a6a4d75c2e7e6178ba99b6d3239fc.png)


[CSDN Blog1](https://blog.csdn.net/a13545564067/article/details/139982318?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522e44955dde814ff36f54716eb661dc0b3%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=e44955dde814ff36f54716eb661dc0b3&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-139982318-null-null.142^v102^pc_search_result_base4&utm_term=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&spm=1018.2226.3001.4187)


[ä¸€æ–‡è¯»æ‡‚è‡ªç¼–ç å™¨(AutoEncoder)](https://blog.csdn.net/hellozhxy/article/details/131184241?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522e44955dde814ff36f54716eb661dc0b3%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=e44955dde814ff36f54716eb661dc0b3&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-131184241-null-null.142^v102^pc_search_result_base4&utm_term=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&spm=1018.2226.3001.4187)

- It has two main parts: **Encoder&Decoder**

```css
  [ Input Image ]
     â†“
   Encoder      ğŸ”’ (compresses the image into a smaller vector)
     â†“
 Latent Code    ğŸ§  (compact representation â€” like "compressed meaning")
     â†“
   Decoder      ğŸ”“ (reconstructs the image from the latent code)
     â†“
[ Reconstructed Image ]
```

## ğŸ¯ Purpose
- The network **learns to keep only the most important information** to **rebuild the original data as closely as possible**
â€” which is exactly **what compression is about!**

## ğŸ”§ Key Components:
- **Encoder**	Maps input to a compressed latent space (e.g., convolutional layers)
- **Latent Space**	Compressed representation of the data
- **Decoder**	Reconstructs the input from the latent vector

## Types of Autoencoder
| Type                     | Main Idea                               | Best For                              |
|--------------------------|------------------------------------------|----------------------------------------|
| Vanilla Autoencoder      | Reconstruct input                        | Basic feature learning                 |
| Denoising Autoencoder    | Reconstruct clean input from noisy       | Noise robustness, denoising            |
| Sparse Autoencoder       | Enforce sparsity in latent layer         | Interpretable features                 |
| Variational Autoencoder  | Learn a distribution in latent space     | Generation, sampling                   |
| Contractive Autoencoder  | Penalize sensitivity to input noise      | Local feature robustness               |
| Convolutional AE         | Use CNNs for encoding/decoding           | Image tasks                            |
| Recurrent Autoencoder    | Use RNNs/LSTMs for sequences             | Time-series, speech, NLP               |
| Seq2Seq Autoencoder      | Encode and decode variable-length input  | NLP, translation, sequence modeling    |
| Adversarial Autoencoder  | Autoencoder + GAN training               | Structured latent space                |
| Attention AE             | Add attention for context sensitivity    | Language/image understanding           |
| Beta/Info/Factor VAE     | Disentangled latent variables            | Generative modeling, interpretability  |



## ğŸ“¦ Usage of Autoencoders
### 1. ğŸ—œï¸ Image Compression
- Learn how to **compress images** into smaller binary codes or latent vectors.
- Unlike JPEG, it's learned from data and can preserve semantic features.

### 2. ğŸ“ˆ Dimensionality Reduction(Feature Extraction)
- Like PCA but **non-linear and learned via deep learning**.
- Useful for visualizing or simplifying high-dimensional data.

### 3. ğŸ­ Denoising
- Train on clean images, and input noisy ones â€” the model learns to "clean" them.
- Used in medical imaging, photos, and signal processing.

### 4. ğŸ§¬ Anomaly Detection
- Train on normal data â†’ model learns what "normal" looks like.
- If reconstruction error is high, it might be an anomaly.

### 5. ğŸ¨ Image Generation
- Part of Variational Autoencoders (VAEs) or GANs for generating new images.

## Example (Image Compression with Autoencoder)
Imagine compressing a 128Ã—128 image into a 16-dimensional latent vector:
- Encoder shrinks it down.
- Decoder rebuilds it from the 16 values.
- If trained well, **the output image looks very similar â€” but the size is tiny**!

## Repositories from Github
### ğŸ–¼ï¸ Image Compression with Autoencoders
#### Learned Image Compression Using Autoencoder Architecture
- Description: Demonstrates a basic architecture for learned image compression, discussing main building blocks, hyperparameters, and comparisons to traditional codecs.
- Repository: [Github](https://github.com/MahmoudAshraf97/AutoencoderCompression?utm_source=chatgpt.com)

#### Image Compression using CNN-based Autoencoders
- Description: Showcases how deep learning can compress images to very low bitrates while retaining high quality, using CNN-based autoencoders.
- Repository: [Github](https://github.com/abskj/lossy-image-compression?utm_source=chatgpt.com)

### ğŸš¨ Anomaly Detection with Autoencoders
#### Anomaly Detection with Autoencoder
- Description: Utilizes autoencoders trained on normal data to identify anomalies that deviate from learned patterns.
- Repository: [Github](https://github.com/AarnoStormborn/anomaly-detection-with-autoencoder?utm_source=chatgpt.com)

#### Anomaly Detection using Autoencoders
- Description: Explores how autoencoders can detect anomalies by learning compressed representations of normal data and identifying deviations.
- Repository: [Github](https://github.com/hellomlorg/Anomaly-Detection-using-Autoencoders?utm_source=chatgpt.com)

### ğŸ§¼ Image Denoising with Autoencoders
#### UNet-based Denoising Autoencoder in PyTorch
- Description: Cleans printed text using a denoising autoencoder based on the UNet architecture, implemented in PyTorch.
- Repository: [Github](https://github.com/n0obcoder/UNet-based-Denoising-Autoencoder-In-PyTorch?utm_source=chatgpt.com)

#### Denoising Autoencoder - Udacity Deep Learning v2 PyTorch
- Description: Demonstrates denoising autoencoders using the MNIST dataset, adding noise to data and training the model to reconstruct clean images.
- Repository: [Github](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/Denoising_Autoencoder_Solution.ipynb?utm_source=chatgpt.com)

## è‡ªç¼–ç å™¨çš„å®ç°ç¤ºä¾‹ï¼ˆä½¿ç”¨TensorFlowå’ŒKerasï¼‰
- ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨TensorFlowå®ç°è‡ªç¼–ç å™¨çš„ç®€å•ç¤ºä¾‹ã€‚è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ä¸ªåŸºæœ¬çš„è‡ªç¼–ç å™¨ï¼Œç”¨äºå›¾åƒæ•°æ®çš„å‹ç¼©å’Œé‡æ„ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç»å…¸çš„MNISTæ‰‹å†™æ•°å­—æ•°æ®é›†ã€‚

```python
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt

# åŠ è½½MNISTæ•°æ®é›†
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# å°†æ•°æ®å±•å¼€ä¸ºä¸€ç»´å‘é‡
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

# å®šä¹‰ç¼–ç å™¨
input_dim = x_train.shape[1]
encoding_dim = 32  # å‹ç¼©åçš„ç»´åº¦

input_img = layers.Input(shape=(input_dim,))
encoded = layers.Dense(encoding_dim, activation='relu')(input_img)

# å®šä¹‰è§£ç å™¨
decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)

# æ„å»ºè‡ªç¼–ç å™¨æ¨¡å‹
autoencoder = models.Model(input_img, decoded)

# æ„å»ºå•ç‹¬çš„ç¼–ç å™¨æ¨¡å‹
encoder = models.Model(input_img, encoded)

# æ„å»ºå•ç‹¬çš„è§£ç å™¨æ¨¡å‹
encoded_input = layers.Input(shape=(encoding_dim,))
decoder_layer = autoencoder.layers[-1]
decoder = models.Model(encoded_input, decoder_layer(encoded_input))

# ç¼–è¯‘è‡ªç¼–ç å™¨æ¨¡å‹
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# è®­ç»ƒè‡ªç¼–ç å™¨
autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))

# ä½¿ç”¨ç¼–ç å™¨å’Œè§£ç å™¨è¿›è¡Œç¼–ç å’Œè§£ç 
encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)

# å¯è§†åŒ–ç»“æœ
n = 10  # æ˜¾ç¤º10ä¸ªæ•°å­—
plt.figure(figsize=(20, 4))
for i in range(n):
    # åŸå§‹å›¾åƒ
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # é‡æ„å›¾åƒ
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
```

è¯´æ˜ï¼š
- æ•°æ®é¢„å¤„ç†: åŠ è½½MNISTæ•°æ®é›†ï¼Œå¹¶å°†å…¶åƒç´ å€¼å½’ä¸€åŒ–åˆ°[0,1]åŒºé—´ã€‚
- æ¨¡å‹æ„å»º:
  - ç¼–ç å™¨: è¾“å…¥å±‚è¿æ¥åˆ°ä¸€ä¸ªéšè—å±‚ï¼ˆç¼–ç å±‚ï¼‰ï¼Œå°†æ•°æ®å‹ç¼©åˆ°32ç»´ã€‚
  - è§£ç å™¨: å°†ç¼–ç åçš„æ•°æ®é‡æ„å›åŸå§‹ç»´åº¦ã€‚
  - è‡ªç¼–ç å™¨: ç¼–ç å™¨å’Œè§£ç å™¨ç»„åˆåœ¨ä¸€èµ·å½¢æˆå®Œæ•´çš„è‡ªç¼–ç å™¨æ¨¡å‹ã€‚
- è®­ç»ƒæ¨¡å‹: ä½¿ç”¨binary_crossentropyæŸå¤±å‡½æ•°å’Œadamä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒã€‚
- ç»“æœå¯è§†åŒ–: æ˜¾ç¤ºåŸå§‹å›¾åƒå’Œé‡æ„å›¾åƒï¼Œä»¥æ¯”è¾ƒå®ƒä»¬çš„ç›¸ä¼¼æ€§ã€‚

è¾“å‡ºï¼š
![Output](https://i-blog.csdnimg.cn/blog_migrate/c6b1dffe4508db7d80017967d3574498.png)



