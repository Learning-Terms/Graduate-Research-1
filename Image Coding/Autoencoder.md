# Autoencoder

## ğŸ§  What is an Autoencoder?
- An Autoencoder is a type of **neural network** used to **compress and reconstruct data** â€” usually **unsupervised (no labels required)**.

- It has two main parts: **Encoder&Decoder**

```css
  [ Input Image ]
     â†“
   Encoder      ğŸ”’ (compresses the image into a smaller vector)
     â†“
 Latent Code    ğŸ§  (compact representation â€” like "compressed meaning")
     â†“
   Decoder      ğŸ”“ (reconstructs the image from the latent code)
     â†“
[ Reconstructed Image ]
```

## ğŸ¯ Purpose
- The network **learns to keep only the most important information** to **rebuild the original data as closely as possible**
â€” which is exactly **what compression is about!**

## ğŸ”§ Key Components:
- **Encoder**	Maps input to a compressed latent space (e.g., convolutional layers)
- **Latent Space**	Compressed representation of the data
- **Decoder**	Reconstructs the input from the latent vector

## ğŸ“¦ Usage of Autoencoders
### 1. ğŸ—œï¸ Image Compression
- Learn how to **compress images** into smaller binary codes or latent vectors.
- Unlike JPEG, it's learned from data and can preserve semantic features.

### 2. ğŸ“ˆ Dimensionality Reduction
- Like PCA but **non-linear and learned via deep learning**.
- Useful for visualizing or simplifying high-dimensional data.

### 3. ğŸ­ Denoising
- Train on clean images, and input noisy ones â€” the model learns to "clean" them.
- Used in medical imaging, photos, and signal processing.

### 4. ğŸ§¬ Anomaly Detection
- Train on normal data â†’ model learns what "normal" looks like.
- If reconstruction error is high, it might be an anomaly.

### 5. ğŸ¨ Image Generation
- Part of Variational Autoencoders (VAEs) or GANs for generating new images.

## Example (Image Compression with Autoencoder)
Imagine compressing a 128Ã—128 image into a 16-dimensional latent vector:
- Encoder shrinks it down.
- Decoder rebuilds it from the 16 values.
- If trained well, **the output image looks very similar â€” but the size is tiny**!

## Repositories from Github
### ğŸ–¼ï¸ Image Compression with Autoencoders
#### Learned Image Compression Using Autoencoder Architecture
- Description: Demonstrates a basic architecture for learned image compression, discussing main building blocks, hyperparameters, and comparisons to traditional codecs.
- Repository: [Github](https://github.com/MahmoudAshraf97/AutoencoderCompression?utm_source=chatgpt.com)

#### Image Compression using CNN-based Autoencoders
- Description: Showcases how deep learning can compress images to very low bitrates while retaining high quality, using CNN-based autoencoders.
- Repository: [Github](https://github.com/abskj/lossy-image-compression?utm_source=chatgpt.com)

### ğŸš¨ Anomaly Detection with Autoencoders
#### Anomaly Detection with Autoencoder
- Description: Utilizes autoencoders trained on normal data to identify anomalies that deviate from learned patterns.
- Repository: [Github](https://github.com/AarnoStormborn/anomaly-detection-with-autoencoder?utm_source=chatgpt.com)

#### Anomaly Detection using Autoencoders
- Description: Explores how autoencoders can detect anomalies by learning compressed representations of normal data and identifying deviations.
- Repository: [Github](https://github.com/hellomlorg/Anomaly-Detection-using-Autoencoders?utm_source=chatgpt.com)

### ğŸ§¼ Image Denoising with Autoencoders
#### UNet-based Denoising Autoencoder in PyTorch
- Description: Cleans printed text using a denoising autoencoder based on the UNet architecture, implemented in PyTorch.
- Repository: [Github](https://github.com/n0obcoder/UNet-based-Denoising-Autoencoder-In-PyTorch?utm_source=chatgpt.com)

#### Denoising Autoencoder - Udacity Deep Learning v2 PyTorch
- Description: Demonstrates denoising autoencoders using the MNIST dataset, adding noise to data and training the model to reconstruct clean images.
- Repository: [Github](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/Denoising_Autoencoder_Solution.ipynb?utm_source=chatgpt.com)





